{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f648b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time as stamp\n",
    "from abc import ABC, abstractmethod\n",
    "\"\"\"ver.2.2\"\"\"\n",
    "\"\"\"추가해야할 것 : 1. Xavier, He 초기화\n",
    "                3. batch normalization\n",
    "                4. early stopping\n",
    "                5. nesterov momentum optimizer\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def OneHot(label_data, num_class):\n",
    "    return np.identity(num_class)[label_data]\n",
    "\n",
    "def Loss_Cross_Entropy(pred_y, true_y, batch_size):\n",
    "    return -np.sum(t * np.log(output_layer.y + 1e-8)) / batch_size\n",
    "\n",
    "def Loss_RMSE(pred_y, true_y):\n",
    "    return np.sqrt((pred_y - true_y)**2) / 2.0\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Model Declaration\"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "        self.LayerList = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \"\"\"Layer adding method\"\"\"\n",
    "        self.LayerList.append(layer)\n",
    "    \n",
    "    def _forward(self, x_data, is_train):\n",
    "        \"\"\"Forward propagation method\"\"\"\n",
    "        for layer in self.LayerList:\n",
    "            if layer.type[1] == 'dropout':\n",
    "                x_data = layer.forward(x_data, is_train)\n",
    "            else:\n",
    "                x_data = layer.forward(x_data)\n",
    "        return x_data\n",
    "    \n",
    "    def _backprop(self, y_data):\n",
    "        \"\"\"Backward propagation method\"\"\"\n",
    "        for layer in self.LayerList[::-1]:\n",
    "            y_data = layer.backprop(y_data)\n",
    "        return y_data\n",
    "    \n",
    "    def _update(self, alpha):\n",
    "        \"\"\"Parameters update method\"\"\"\n",
    "        for layer in self.LayerList:\n",
    "            layer.update(alpha)\n",
    "            \n",
    "    def fit(self, X_train, y_train, X_test, y_test, batch_size, epochs, alpha=0.01):\n",
    "        \"\"\"Fitting function\"\"\"\n",
    "        num_train = X_train.shape[0]\n",
    "        num_test = X_test.shape[0]\n",
    "        num_batch = num_train // batch_size\n",
    "        \n",
    "        self._forward(X_train, False)\n",
    "        print(f\"[0/{epochs} epochs] \",end='')\n",
    "        self.score(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        t0 = stamp()\n",
    "        for epoch in range(epochs):\n",
    "            rand_idx = np.arange(num_train)\n",
    "            np.random.shuffle(rand_idx)\n",
    "            for mini_batch in range(num_batch):\n",
    "                mb_index = rand_idx[mini_batch*batch_size:(mini_batch + 1)*batch_size]\n",
    "                x = X_train[mb_index, :]\n",
    "                y = y_train[mb_index, :]\n",
    "                \n",
    "                self._forward(x, True)\n",
    "                self._backprop(y)\n",
    "                self._update(alpha)\n",
    "            \n",
    "            print(f\"[{epoch+1}/{epochs} epochs] \",end='')\n",
    "            self.score(X_train, y_train, X_test, y_test)\n",
    "            \n",
    "        t1 = stamp()\n",
    "        total_time = t1-t0\n",
    "        minute = total_time // 60\n",
    "        second = round(total_time % 60, 2)\n",
    "        \n",
    "        print(f\"Training complete! {minute}minutes, {second}seconds\")\n",
    "        \n",
    "    def score(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Score method\"\"\"\n",
    "        num_train = X_train.shape[0]\n",
    "        num_test = X_test.shape[0]\n",
    "        \n",
    "        predict_y1 = self._forward(X_train, False)\n",
    "        count_train = np.sum(np.argmax(predict_y1, axis=1) == np.argmax(y_train, axis=1))\n",
    "\n",
    "        predict_y2 = self._forward(X_test, False)\n",
    "        count_test = np.sum(np.argmax(predict_y2, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "        print(f\"Train Accuracy : {round(count_train/num_train*100,3)}%,\",\n",
    "              f\"Test Accuracy : {round(count_test/num_test*100,3)}%\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict method\"\"\"\n",
    "        return self._forward(x, False)\n",
    "    \n",
    "    def total_parameters(self):\n",
    "        self.total_parameters = 0\n",
    "        for layer in self.LayerList:\n",
    "            if layer.type != 'Drop':\n",
    "                self.total_parameters += layer.W.size + layer.b.size\n",
    "        return self.total_parameters\n",
    "    \n",
    "class Activation(ABC):\n",
    "    \"\"\"Activation function Abstract Class\"\"\"\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backprop(self):\n",
    "        pass\n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'sigmoid')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * (np.exp(-self.x)) / ((np.exp(-self.x)+1)**2)\n",
    "    \n",
    "class Linear(Activation):\n",
    "    \"\"\"Linear activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'linear')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = x\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy\n",
    "    \n",
    "class ReLU(Activation):\n",
    "    \"\"\"Reductified Linear Unit activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.where(self.x > 0, x, 0)\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * np.where(self.x > 0, 1, 0)\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'softmax')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x - np.max(x)\n",
    "            self.pred_y = np.exp(x) / np.sum(np.exp(x))\n",
    "            return self.pred_y\n",
    "        \n",
    "        elif x.ndim == 2:\n",
    "            x = x - np.max(x, axis=1).reshape(-1,1)\n",
    "            self.pred_y = np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1,1)\n",
    "            return self.pred_y\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"꺆꺆꺆\")\n",
    "            \n",
    "    def backprop(self, true_y):\n",
    "        return self.pred_y - true_y\n",
    "\n",
    "class Leaky_ReLU(Activation):\n",
    "    \"\"\"Leaky ReLU activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'leaky_relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.where(self.x > 0, x, 0.01 * x)\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * np.where(self.x > 0, 1, 0.01)\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Exponential Linear Unit activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'elu')\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.where(self.x > 0, x, self.alpha * (np.exp(self.x)-1))\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * np.where(self.x > 0, 1, self.alpha * np.exp(self.x))\n",
    "\n",
    "class tanh(Activation):\n",
    "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('activation', 'tanh')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * (1 - self.y) * (1 + self.y)\n",
    "    \n",
    "class PReLU(Activation):\n",
    "    \"\"\"Parametric ReLU activation function\"\"\"\n",
    "    def __init__(self, alpha=0.05):\n",
    "        self.type = ('activation', 'prelu')\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.where(self.x > 0, x, self.alpha * x)\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        return dy * np.where(self.x > 0, 1, self.alpha)\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    \"\"\"Optimizer Abstract Class\"\"\"\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, alpha):\n",
    "        pass\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('optimizer', 'SGD')\n",
    "    \n",
    "    def update(self, params, grads, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.alpha * grads[key]\n",
    "            \n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"Momentum optimizer\"\"\"\n",
    "    def __init__(self, momentum=0.9):\n",
    "        self.type = ('optimizer', 'momentum')\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.alpha * grads[key]\n",
    "            params[key] += self.v[key]\n",
    "            \n",
    "class AdaGrad(Optimizer):\n",
    "    \"\"\"Adaptive Gradient optimizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.type = ('optimizer', 'adagrad')\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.alpha * grads[key] / (np.sqrt(self.h[key]) + 1e-8)\n",
    "            \n",
    "class RMSprop(Optimizer):\n",
    "    \"\"\"Root Mean Square Propagation optimizer\"\"\"\n",
    "    def __init__(self, rho=0.99):\n",
    "        self.type = ('optimizer', 'RMSprop')\n",
    "        self.rho = rho\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += self.rho\n",
    "            self.h[key] += (1 - self.rho) * grads[key] * grads[key]\n",
    "            params[key] -= self.alpha * grads[key] / (np.sqrt(self.h[key]) + 1e-8)\n",
    "            \n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adapive Moment esimation optimizer\"\"\"\n",
    "    def __init__(self, beta1=0.9, beta2=0.999):\n",
    "        self.type = ('optimizer', 'adam')\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self,params, grads, alpha=0.001):\n",
    "        self.alpha = alpha\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        self.iter += 1\n",
    "        alpha_t = self.alpha * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= alpha_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-8)\n",
    "\n",
    "class Layer(ABC):\n",
    "    \"\"\"Layer Abstract Class\"\"\"\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backprop(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, alpha):\n",
    "        pass\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Fully Connected Layer\"\"\"\n",
    "    def __init__(self, pre_neurons, neurons, name=None, activation='relu',\n",
    "                 optimizer='Adam'):\n",
    "        self.W = np.random.randn(pre_neurons, neurons) * np.sqrt(1.0 / neurons)\n",
    "        self.b = np.random.randn(neurons)\n",
    "        self.params = {'Weight' : self.W, 'bias' : self.b}\n",
    "        self.grads = {}\n",
    "        \n",
    "        self.name = name\n",
    "        self.type = ('layer','dense')\n",
    "        \n",
    "        self.activation_dict = {'sigmoid' : Sigmoid(),\n",
    "                                'relu' : ReLU(),\n",
    "                                'softmax' : Softmax(),\n",
    "                                'linear' : Linear(),\n",
    "                                'elu' : ELU(),\n",
    "                                'leakey_relu' : Leaky_ReLU(),\n",
    "                                'prelu' : PReLU(),\n",
    "                                'tanh' : tanh()}\n",
    "        \n",
    "        self.optimizer_dict = {'SGD' : SGD(),\n",
    "                               'Momentum' : Momentum(),\n",
    "                               'Adagrad' : AdaGrad(),\n",
    "                               'RMSprop' : RMSprop(),\n",
    "                               'Adam' : Adam()}\n",
    "        \n",
    "        self.activation = self.activation_dict[activation]\n",
    "        self.optimizer = self.optimizer_dict[optimizer]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = self.x @ self.W + self.b\n",
    "        self.y = self.activation.forward(self.z)\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dy):\n",
    "        delta = self.activation.backprop(dy)\n",
    "\n",
    "        self.dW = self.x.T @ delta\n",
    "        self.db = np.sum(delta, axis=0)    \n",
    "        self.dx = delta @ self.W.T\n",
    "        \n",
    "        self.grads = {'Weight' : self.dW, 'bias' : self.db}\n",
    "        \n",
    "        return self.dx\n",
    "    \n",
    "    def update(self, alpha):\n",
    "        self.optimizer.update(self.params, self.grads, alpha)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Print weight and bias\"\"\"\n",
    "        return (f\"{self.name}'s Layer W\\n{self.W}\\n\\n{self.name}'s Layer b\\n{self.b}\")\n",
    "    \n",
    "class Dropout(Layer):\n",
    "    \"\"\"Dropout layer\"\"\"\n",
    "    def __init__(self, dropout_ratio, name=None):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.name = name\n",
    "        self.type = ('layer','dropout')\n",
    "        \n",
    "    def forward(self, x, is_train):\n",
    "        if is_train:\n",
    "            rand = np.random.rand(*x.shape)\n",
    "            self.dropout = np.where(rand > self.dropout_ratio, 1, 0)\n",
    "            self.y = x * self.dropout\n",
    "        else:\n",
    "            self.y = (1-self.dropout_ratio)*x\n",
    "        return self.y\n",
    "        \n",
    "    def backprop(self, dy):\n",
    "        self.dx = dy * self.dropout\n",
    "        return self.dx\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Print Layer feature\"\"\"\n",
    "        return (f\"{self.name}'s dropout ratio is {self.dropout_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "np.random.seed(26)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:4000].reshape(-1,28*28).astype(np.float32)/255\n",
    "X_test = X_test[:1000].reshape(-1,28*28).astype(np.float32)/255\n",
    "\n",
    "num_class = 10\n",
    "y_train = y_train[:4000]\n",
    "y_test = y_test[:1000]\n",
    "y_train = OneHot(y_train, num_class)\n",
    "y_test = OneHot(y_test, num_class)\n",
    "\n",
    "#plt.imshow(X_train[0].reshape(28,28),cmap='gray')\n",
    "\n",
    "n_in = 784  \n",
    "n_mid1 = 200\n",
    "n_mid2 = 100\n",
    "n_out = 10\n",
    "\n",
    "model = Model()\n",
    "a = Dense(n_in, n_mid1, optimizer='Adam', activation = 'elu', name = 'D1')\n",
    "model.add(a)\n",
    "model.add(Dropout(0.5, name = 'P1'))\n",
    "model.add(Dense(n_mid1, n_mid2, optimizer='Adam', activation = 'elu',name = 'D2'))\n",
    "model.add(Dropout(0.5, name = 'P2'))\n",
    "model.add(Dense(n_mid2, n_out, optimizer='Adam', activation = 'softmax',name = 'out'))\n",
    "model.fit(X_train, y_train, X_test, y_test, batch_size=20, epochs=10, alpha=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
